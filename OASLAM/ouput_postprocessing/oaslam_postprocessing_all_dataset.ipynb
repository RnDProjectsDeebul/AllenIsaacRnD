{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e56c381",
   "metadata": {},
   "source": [
    "**Given the path to the folder containing all the datasets(dataset_path), this script will align the slam output with the ground truth and perform the error metrics estimation and generate a file containing the error metrics.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdf547",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af4b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import math\n",
    "from matplotlib.patches import Patch\n",
    "from distinctipy import get_colors # to get unique colors\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "from itertools import product\n",
    "from numpy.linalg import LinAlgError, eigvalsh\n",
    "from scipy.spatial.transform import Rotation\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# save matplotlib plots without displaying\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525748b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa695ae3",
   "metadata": {},
   "source": [
    "# Global Data and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b58356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOP YCBV dataset\n",
    "dataset_info = {'1': 'master_chef_can',\n",
    "                '2': 'cracker_box',\n",
    "                '3': 'sugar_box',\n",
    "                '4': 'tomato_soup_can',\n",
    "                '5': 'mustard_bottle',\n",
    "                '6': 'tuna_fish_can',\n",
    "                '7': 'pudding_box',\n",
    "                '8': 'gelatin_box',\n",
    "                '9': 'potted_meat_can',\n",
    "                '10': 'banana',\n",
    "                '11': 'pitcher_base',\n",
    "                '12': 'bleach_cleanser',\n",
    "                '13': 'bowl',\n",
    "                '14': 'mug',\n",
    "                '15': 'power_drill',\n",
    "                '16': 'wood_block',\n",
    "                '17': 'scissors',\n",
    "                '18': 'large_marker',\n",
    "                '19': 'large_clamp',\n",
    "                '20': 'extra_large_clamp',\n",
    "                '21': 'foam_brick'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6569818",
   "metadata": {},
   "source": [
    "## OA-SLAM dual quadrics decomposition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0b4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the dual quadric matrix representation\n",
    "# ouputs -> 4x4 transformation matrix, radius\n",
    "def decompose(dual_quadrics:np.ndarray):\n",
    "    \n",
    "    center = -dual_quadrics[:, 3][:3]\n",
    "\n",
    "    T_c = np.eye(4)\n",
    "    T_c[0, 3] = -center[0]\n",
    "    T_c[1, 3] = -center[1]\n",
    "    T_c[2, 3] = -center[2]\n",
    "    temp = T_c @ dual_quadrics @ T_c.T\n",
    "    pose_center = 0.5 * (temp + temp.T)\n",
    "\n",
    "    try:\n",
    "        eig_values = eigvalsh(pose_center[:3, :3])\n",
    "        eig_vectors = np.linalg.eigh(pose_center[:3, :3])[1]\n",
    "    except LinAlgError:\n",
    "        raise ValueError(\"Matrix is not positive definite. Decomposition failed.\")\n",
    "\n",
    "    if np.linalg.det(eig_vectors) < 0.0:\n",
    "        eig_vectors[:, 2] *= -1\n",
    "\n",
    "    axes = np.sqrt(np.abs(eig_values))\n",
    "    R = eig_vectors\n",
    "    has_changed = False\n",
    "    \n",
    "    pose = np.eye(4)\n",
    "    pose[:3, -1] = T_c[:3, -1]\n",
    "    pose[:3, :3] = R\n",
    "    \n",
    "    return pose, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82b409",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb1dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(data:list, low_percentile:int, upp_percentile:int):\n",
    "\n",
    "    # Calculate the median and IQR\n",
    "    median = np.median(data)\n",
    "    q1 = np.percentile(data, low_percentile)\n",
    "    q3 = np.percentile(data, upp_percentile)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define a threshold for outlier detection\n",
    "    threshold = 1.5\n",
    "\n",
    "    # Identify potential outliers\n",
    "    potential_outliers = [x for x in data if x < q1 - threshold * iqr or x > q3 + threshold * iqr]\n",
    "\n",
    "    return potential_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e897a",
   "metadata": {},
   "source": [
    "## Function to plot ellipsoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833d60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - https://github.com/qcr/quadricslam/blob/master/src/quadricslam/visualisation.py\n",
    "def plot_ellipsoid(pose: np.ndarray, radii: np.ndarray, ax: matplotlib.figure.Axes, color):\n",
    "    # Generate ellipsoid of appropriate size at origin\n",
    "    SZ = 50\n",
    "    radii = np.abs(radii)\n",
    "    u, v = np.linspace(0, 2 * np.pi, SZ), np.linspace(0, np.pi, SZ)\n",
    "    x, y, z = (radii[0] * np.outer(np.cos(u), np.sin(v)),\n",
    "               radii[1] * np.outer(np.sin(u), np.sin(v)),\n",
    "               radii[2] * np.outer(np.ones_like(u), np.cos(v)))\n",
    "\n",
    "    # Rotate the ellipsoid, then translate to centroid\n",
    "    ps = pose @ np.vstack([\n",
    "        x.reshape(-1),\n",
    "        y.reshape(-1),\n",
    "        z.reshape(-1),\n",
    "        np.ones(z.reshape(-1).shape)\n",
    "    ])\n",
    "\n",
    "    # Plot the ellipsoid\n",
    "    ax.plot_wireframe(\n",
    "        ps[0, :].reshape(SZ, SZ),\n",
    "        ps[1, :].reshape(SZ, SZ),\n",
    "        ps[2, :].reshape(SZ, SZ),\n",
    "        rstride=4,\n",
    "        cstride=4,\n",
    "        color=color,\n",
    "        linewidth=0.5,\n",
    "    )## Function to plot ellipsoids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f27764",
   "metadata": {},
   "source": [
    "## Function to plot cuboids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7afd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cuboid(pose: np.ndarray, size: np.ndarray, ax: matplotlib.figure.Axes, color):\n",
    "    # Get all 8 corner points of the cuboid\n",
    "    #vertices = np.array(list(product(*zip(pose[:3, -1] - 0.5 * size, pose[:3, -1] + 0.5 * size))))\n",
    "    vertices = np.array(list(product(*zip(- 0.5 * size, + 0.5 * size))))\n",
    "    \n",
    "    # Transform the cuboid's vertices using the pose matrix\n",
    "    t_vertices = np.dot(pose[:3, :3], vertices.T).T + pose[:3, -1]\n",
    "    \n",
    "    # Define the edges of the cuboid using the vertices\n",
    "    edges = [\n",
    "        [t_vertices[0], t_vertices[1]], [t_vertices[1], t_vertices[5]], [t_vertices[5], t_vertices[4]],\n",
    "        [t_vertices[4], t_vertices[0]], [t_vertices[7], t_vertices[6]], [t_vertices[6], t_vertices[2]],\n",
    "        [t_vertices[2], t_vertices[3]], [t_vertices[3], t_vertices[7]], [t_vertices[0], t_vertices[2]],\n",
    "        [t_vertices[1], t_vertices[3]], [t_vertices[4], t_vertices[6]], [t_vertices[5], t_vertices[7]]\n",
    "    ]\n",
    "    \n",
    "    ax.add_collection3d(Line3DCollection(edges, colors=color, linewidths=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14954b12",
   "metadata": {},
   "source": [
    "## Function to plot trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbf0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(data: List, ax: matplotlib.figure.Axes, length = 50, c:str = 'cyan', label = None):\n",
    "    \n",
    "    # to plot trajectory\n",
    "    traj = []\n",
    "    \n",
    "    for idx in range(len(data)):\n",
    "        # position\n",
    "#         ax.scatter(data[idx][0,-1], data[idx][1,-1], data[idx][2,-1], c='r', marker='*')\n",
    "        # orientation\n",
    "        if idx % 30 == 0:\n",
    "            # X\n",
    "            ax.quiver(data[idx][0,-1], data[idx][1,-1], data[idx][2,-1],\n",
    "                      data[idx][0, 0], data[idx][1, 0], data[idx][2, 0],\n",
    "                      color='r', length=length, linewidth=0.2, alpha=1)\n",
    "            # Y\n",
    "            ax.quiver(data[idx][0,-1], data[idx][1,-1], data[idx][2,-1],\n",
    "                      data[idx][0, 1], data[idx][1, 1], data[idx][2, 1],\n",
    "                      color='g', length=length, linewidth=0.2, alpha=1)\n",
    "            # Z\n",
    "            ax.quiver(data[idx][0,-1], data[idx][1,-1], data[idx][2,-1],\n",
    "                      data[idx][0, 2], data[idx][1, 2], data[idx][2, 2],\n",
    "                      color='b', length=length, linewidth=0.2, alpha=1)\n",
    "        \n",
    "        traj.append([data[idx][0,-1], data[idx][1,-1], data[idx][2,-1]])\n",
    "        \n",
    "    #trajectory\n",
    "    if c != 'cyan':\n",
    "        traj = np.array(traj)\n",
    "        if label==None:\n",
    "            ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=c)\n",
    "        else:\n",
    "            ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=c, label = label)\n",
    "        \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca012357",
   "metadata": {},
   "source": [
    "# Functions for Error Calculation of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499cd7a9",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695e8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given 2 centroids, it will output the euclidean distance between the 2 points\n",
    "def euclidean_distance(centroid1, centroid2):\n",
    "    return np.linalg.norm(centroid1 - centroid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6d09",
   "metadata": {},
   "source": [
    "### Rotation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16812f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rotation matrix to quaternion conversion\n",
    "def matrix_to_quaternion(matrix):\n",
    "    # Extract the 3x3 rotation matrix from the 4x4 transformation matrix\n",
    "    rotation_matrix = matrix[:3, :3]\n",
    "\n",
    "    # Ensure the rotation matrix is orthogonal (if necessary)\n",
    "    if not np.allclose(np.linalg.det(rotation_matrix), 1.0):\n",
    "        rotation_matrix = rotation_matrix / np.cbrt(np.linalg.det(rotation_matrix))\n",
    "\n",
    "    # Create a Rotation object from the rotation matrix\n",
    "    rotation = Rotation.from_matrix(rotation_matrix)\n",
    "\n",
    "    # Get the quaternion representation\n",
    "    quaternion = rotation.as_quat()\n",
    "\n",
    "    return quaternion\n",
    "\n",
    "\n",
    "## quaternion representation of the rotation matrix as input\n",
    "def rotation_error_quaternion(quat1, quat2):\n",
    "    r1 = Rotation(quat1)\n",
    "    r2 = Rotation(quat2)\n",
    "\n",
    "    # Convert to unit quaternions if necessary\n",
    "    if not np.isclose(np.linalg.norm(r1.as_quat()), 1.0):\n",
    "        r1 = r1.normalized()\n",
    "    if not np.isclose(np.linalg.norm(r2.as_quat()), 1.0):\n",
    "        r2 = r2.normalized()\n",
    "    # Compute the rotation error (angle between the quaternions)\n",
    "    error_angle = 2.0 * np.arccos(np.abs(np.dot(r1.as_quat(), r2.as_quat())))\n",
    "    return error_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ecb903",
   "metadata": {},
   "source": [
    "### Procrustes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc8ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_analysis(points1, points2):\n",
    "    set1, set2, disparity = procrustes(points1, points2)\n",
    "    return set1, set2, disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d71601",
   "metadata": {},
   "source": [
    "### Fréchet Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908a7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frechet_distance(curve1, curve2):\n",
    "    \n",
    "    # Compute the directed Hausdorff distance (equivalent to Fréchet Distance)\n",
    "    distance_1_to_2, _, _ = directed_hausdorff(curve1, curve2)\n",
    "    distance_2_to_1, _, _ = directed_hausdorff(curve2, curve1)\n",
    "\n",
    "    # Return the maximum of the two directed Hausdorff distances (Fréchet Distance)\n",
    "    frechet_distance = max(distance_1_to_2, distance_2_to_1)\n",
    "    return frechet_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7091593",
   "metadata": {},
   "source": [
    "### Chamfer Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b93c4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_distance(point_cloud1, point_cloud2):\n",
    "    return np.mean(np.min(cdist(point_cloud1, point_cloud2),axis=0)) + np.mean(np.min(cdist(point_cloud2, point_cloud1), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd3323",
   "metadata": {},
   "source": [
    "### Percentage of Intersection Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912b54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate points within GT cuboid\n",
    "def generate_points_within_cuboid(centroid, dimensions, rotation_matrix, num_points=1000):\n",
    "    # Extract centroid coordinates\n",
    "    cx, cy, cz = centroid\n",
    "    \n",
    "    # Extract dimensions of the cuboid\n",
    "    Lx, Ly, Lz = dimensions\n",
    "    \n",
    "    # Generate random points within the unit cube\n",
    "    points_in_unit_cube = np.random.rand(num_points, 3)\n",
    "    \n",
    "    # Scale and translate points to fit within the cuboid\n",
    "    points_in_cuboid = points_in_unit_cube * np.array([Lx, Ly, Lz]) - np.array([Lx / 2, Ly / 2, Lz / 2])\n",
    "    \n",
    "    # Apply the rotation to the points\n",
    "    rotated_points = np.dot(points_in_cuboid, rotation_matrix.T)\n",
    "    \n",
    "    # Translate the points to the cuboid centroid\n",
    "    translated_points = rotated_points + np.array([cx, cy, cz])\n",
    "    \n",
    "    return translated_points\n",
    "\n",
    "# function to check if the point lies within the ellipsoid\n",
    "def is_point_inside_ellipsoid(point, centroid, radii, rotation_matrix):\n",
    "    # Convert the point to the ellipsoid's local coordinate system\n",
    "    local_point = point - centroid\n",
    "\n",
    "    # Apply the inverse rotation to bring the point to the ellipsoid's local orientation\n",
    "    local_point_rotated = np.dot(rotation_matrix.T, local_point)\n",
    "\n",
    "    # Normalize the point by dividing its coordinates by the semi-axes lengths of the ellipsoid\n",
    "    normalized_point = local_point_rotated / radii\n",
    "\n",
    "    # Check if the normalized point lies within the unit sphere\n",
    "    return np.linalg.norm(normalized_point) <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142228c",
   "metadata": {},
   "source": [
    "### Rotation Error Correction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd48bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_to_closest_multiple_of_90(number):\n",
    "    # Calculate the remainder when dividing the number by 90\n",
    "    remainder = number % 90\n",
    "    \n",
    "    # Determine whether to round up or down based on the remainder\n",
    "    if remainder < 45:\n",
    "        return number - remainder\n",
    "    else:\n",
    "        return number + (90 - remainder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab950acf",
   "metadata": {},
   "source": [
    "# Function for Postprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c68210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset -> path to the dataset folder\n",
    "# plottings -> the bool that controls the plots to be saved or not\n",
    "def postprocessing(dataset:str, plottings:bool):\n",
    "    \n",
    "    # directory to save images\n",
    "    images_save_directory = os.path.join(dataset, 'oa_slam_result/images')\n",
    "    \n",
    "    if not os.path.exists(images_save_directory):\n",
    "        os.makedirs(images_save_directory)\n",
    "    \n",
    "    \n",
    "    ################# GROUND TRUTH CAMERA POSE #################\n",
    "    ## reading the ground truth scene_camera.json file\n",
    "    f = open(dataset + '/scene_camera.json')\n",
    "    camera_pose = json.load(f)\n",
    "    \n",
    "    ## used for plotting\n",
    "    x_min = 0\n",
    "    x_max = 0\n",
    "    y_min = 0\n",
    "    y_max = 0\n",
    "    z_min = 0\n",
    "    z_max = 0\n",
    "    \n",
    "    ## The camera poses are in the format w2c(World with respect to camera)\n",
    "    ## We need c2w(Camera with respect to world)\n",
    "    \n",
    "    gt_traj = [] ## to store the c2w as 4x4 numpy arrays within a list \n",
    "\n",
    "    for c in camera_pose:\n",
    "        \n",
    "        # converting from json raw format to a numpy array\n",
    "        world2camera = np.column_stack((np.array(camera_pose[c]['cam_R_w2c']).reshape(3,3),\n",
    "                                        np.array(camera_pose[c]['cam_t_w2c']).reshape(3,1)))\n",
    "        world2camera = np.row_stack((world2camera, np.array([0. , 0., 0., 1.])))\n",
    "\n",
    "        # In the dataset, the transformation is w2c(world with respect to the camera)\n",
    "        # we need to change it to camera with respect to the world.\n",
    "        corrected_pose = np.linalg.inv(world2camera)\n",
    "        gt_traj.append(corrected_pose)\n",
    "        \n",
    "        x_min = min(x_min, corrected_pose[0,-1])\n",
    "        x_max = max(x_max, corrected_pose[0,-1])\n",
    "        y_min = min(y_min, corrected_pose[1,-1])\n",
    "        y_max = max(y_max, corrected_pose[1,-1])\n",
    "        z_min = min(z_min, corrected_pose[2,-1])\n",
    "        z_max = max(z_max, corrected_pose[2,-1])\n",
    "    ################# GROUND TRUTH CAMERA POSE #################\n",
    "    \n",
    "    \n",
    "    ################# GROUND TRUTH OBJECT POSE #################\n",
    "    ## reading the ground truth scene_gt.json file\n",
    "    f = open(dataset + '/scene_gt.json')\n",
    "    object_pose = json.load(f)\n",
    "    \n",
    "    ## Loading ground truth obejct models and assigning colors\n",
    "    f = open(os.path.dirname(dataset) + '/models_info.json')\n",
    "    model_data = json.load(f)\n",
    "\n",
    "    classes = [] ## which all classes are present in the current datset\n",
    "\n",
    "    for o in range(len(object_pose['1'])):\n",
    "        classes.append(object_pose['1'][o]['obj_id'])\n",
    "    \n",
    "    # get unique colors for each class\n",
    "    colors = get_colors(len(object_pose['1']))    \n",
    "    \n",
    "    # to store all object poses\n",
    "    gt_pose_data_all = [[] for _ in range(len(object_pose['1']))]\n",
    "\n",
    "    idx = 0\n",
    "    for c in camera_pose:\n",
    "\n",
    "        for o in range(len(object_pose['1'])):\n",
    "\n",
    "            object2camera = np.column_stack((np.array(object_pose[c][o]['cam_R_m2c']).reshape(3,3),\n",
    "                                             np.array(object_pose[c][o]['cam_t_m2c']).reshape(3,1)))\n",
    "            object2camera = np.row_stack((object2camera, np.array([0. , 0., 0.,1.])))\n",
    "\n",
    "            # given W with respect to C and M with respect to C\n",
    "            # We need M with respect to W. In other words,\n",
    "            # we need W_T_M = inv(C_T_W) @ C_T_M = W_T_C @ C_T_M \n",
    "            gt_pose_data_all[o].append(np.dot(gt_traj[idx], object2camera))\n",
    "\n",
    "        idx+=1\n",
    "    \n",
    "    # averaging and getting a single position for the object pose\n",
    "    gt_pose = [] # to store the pose of each object\n",
    "\n",
    "    for obj in gt_pose_data_all:\n",
    "        arrays_stack = np.stack(obj)\n",
    "        averaged_pose = np.mean(arrays_stack, axis=0)\n",
    "        gt_pose.append(averaged_pose)\n",
    "        \n",
    "        x_min = min(x_min, averaged_pose[0,-1])\n",
    "        x_max = max(x_max, averaged_pose[0,-1])\n",
    "        y_min = min(y_min, averaged_pose[1,-1])\n",
    "        y_max = max(y_max, averaged_pose[1,-1])\n",
    "        z_min = min(z_min, averaged_pose[2,-1])\n",
    "        z_max = max(z_max, averaged_pose[2,-1])\n",
    "    \n",
    "    ################# GROUND TRUTH OBJECT POSE #################\n",
    "    \n",
    "    \n",
    "    ################# GROUND TRUTH SCENE PLOTTING #################\n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X Axis')\n",
    "        ax.set_ylabel('Y Axis')\n",
    "        ax.set_zlabel('Z Axis')\n",
    "        ax.set_xlim((x_min-100, x_max+100))\n",
    "        ax.set_ylim((y_min-100, y_max+100))\n",
    "        ax.set_zlim((z_min-100, z_max+100))\n",
    "        plot_traj(gt_traj, ax, c = 'blue', length=200)\n",
    "\n",
    "        # plotting as poses\n",
    "        for o in range(len(object_pose['1'])):\n",
    "            plot_traj([gt_pose[o]], ax, length=200)\n",
    "\n",
    "\n",
    "        # plotting as ellipsoids and cuboids\n",
    "        for o in range(len(object_pose['1'])):\n",
    "            class_id = str(classes[o])\n",
    "            radii = np.array([model_data[class_id]['size_x'],\n",
    "                              model_data[class_id]['size_y'],\n",
    "                              model_data[class_id]['size_z']])\n",
    "            # plotting as ellipsoids\n",
    "            # plot_ellipsoid(gt_pose[o], radii/2, ax, colors[o])\n",
    "            # plotting as cuboids\n",
    "            plot_cuboid(gt_pose[o], radii, ax, colors[o])\n",
    "\n",
    "        ax.legend(handles=[\n",
    "                Patch(facecolor=c, edgecolor=c, label=dataset_info[str(l)]) for l, c in zip(classes, colors)\n",
    "            ])\n",
    "        ax.grid(False)\n",
    "        plt.savefig(images_save_directory + '/ground_truth_scene.png')\n",
    "        plt.close()\n",
    "        \n",
    "    ################# GROUND TRUTH SCENE PLOTTING #################\n",
    "    \n",
    "    ################# ESTIMATED - ALIGNED POSES #################\n",
    "    \n",
    "    ## Estimated Camera Poses from camera_poses_sink.txt\n",
    "    output_data_path = dataset + \"/oa_slam_result/\"\n",
    "    \n",
    "    unaligned_traj = []\n",
    "\n",
    "    dataset_id = os.path.basename(os.path.dirname(output_data_path[:-1]))\n",
    "\n",
    "    with open(output_data_path + 'camera_poses_' + dataset_id + '.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            l = line.strip()\n",
    "            as_float = [float(e) for e in l.split()]\n",
    "            as_float = as_float[1:]\n",
    "            as_nparray = np.array(as_float).reshape(3,4)\n",
    "            as_nparray = np.vstack((as_nparray, np.array([0, 0, 0, 1])))\n",
    "            unaligned_traj.append(as_nparray)\n",
    "    \n",
    "    ## Estimated Object Poses from map_objects_sink.txt\n",
    "    unaligned_quad = []\n",
    "\n",
    "    # QUADRIC ID, CATEGORY ID, 4X4 matrix\n",
    "    with open(output_data_path + 'map_objects_' + dataset_id + '.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            l = line.strip()\n",
    "            as_float = [float(e) for e in l.split()]\n",
    "            as_nparray = np.array(as_float[2:]).reshape(4,4)\n",
    "            temp = dict()\n",
    "            temp['quadric_id'] = as_float[0]\n",
    "            temp['category_id'] = as_float[1]\n",
    "            temp['pose'] = as_nparray\n",
    "            temp['pose'][:, -1] = temp['pose'][:, -1] * (-1)\n",
    "            unaligned_quad.append(temp)\n",
    "\n",
    "\n",
    "    unaligned_quad = sorted(unaligned_quad, key=lambda x: x['category_id'])\n",
    "\n",
    "    ##  to correct the order of the quadrics\n",
    "    sorted_indices = sorted(range(len(classes)), key=lambda i: classes[i])\n",
    "\n",
    "    final = [i for i in range(len(unaligned_quad))]\n",
    "    for i in range(len(unaligned_quad)):\n",
    "        final[sorted_indices[i]] = unaligned_quad[i]\n",
    "\n",
    "    unaligned_quad = final\n",
    "    \n",
    "    \n",
    "    ### Aligning the estimated poses with the ground truth poses\n",
    "    \n",
    "    ## Aligning the GT camera pose with origin\n",
    "    origin_gt_traj = []\n",
    "\n",
    "    gt_traj_origin = np.linalg.inv(gt_traj[0])\n",
    "\n",
    "    for traj in gt_traj:\n",
    "        origin_gt_traj.append(np.dot(gt_traj_origin, traj))\n",
    "        \n",
    "    ## Aligning the estimated camera pose with origin\n",
    "    origin_aligned_traj = []\n",
    "\n",
    "    est_traj_origin = np.linalg.inv(unaligned_traj[0])\n",
    "\n",
    "    for traj in unaligned_traj:\n",
    "        origin_aligned_traj.append(np.dot(est_traj_origin, traj))\n",
    "        \n",
    "    ## Aligning the GT object pose with origin\n",
    "    origin_gt_quad = []\n",
    "\n",
    "    for pose in gt_pose:\n",
    "        origin_gt_quad.append(np.dot(gt_traj_origin, pose))\n",
    "    \n",
    "    ## Aligning the estimated object pose with origin\n",
    "    origin_aligned_quad = []\n",
    "\n",
    "    for o in range(len(unaligned_quad)):\n",
    "        temp = dict()\n",
    "\n",
    "        temp['quadric_id'] = unaligned_quad[o]['quadric_id']\n",
    "\n",
    "        class_id = unaligned_quad[o]['category_id']\n",
    "        temp['category_id'] = class_id\n",
    "\n",
    "        # pose, radii\n",
    "        pose, radii = decompose(unaligned_quad[o]['pose'])\n",
    "        temp['pose'] = np.dot(est_traj_origin, pose)\n",
    "        temp['radii'] = radii\n",
    "\n",
    "        origin_aligned_quad.append(temp)\n",
    "        \n",
    "    ### Finding the scaling factor\n",
    "    ## USING OBJECT DIMENSIONS\n",
    "    ## stores the 3 Radii of each GT object as a numpy array within a list\n",
    "    gt_sizes = []\n",
    "\n",
    "    for o in range(len(gt_pose)):\n",
    "        class_id = str(classes[o])\n",
    "        radii = np.array([model_data[class_id]['size_x']/2,\n",
    "                         model_data[class_id]['size_y']/2,\n",
    "                         model_data[class_id]['size_z']/2])\n",
    "        gt_sizes.append(radii)\n",
    "\n",
    "\n",
    "    ## stores the 3 Radii of each estimated object as a numpy array within a list\n",
    "    est_sizes = []\n",
    "    for o in range(len(unaligned_quad)):\n",
    "        _, axes = decompose(unaligned_quad[o]['pose'])\n",
    "        est_sizes.append(axes)\n",
    "\n",
    "\n",
    "    ## divide the radii of the ground truth object with the estimated object\n",
    "    ## take the average of the 3 ratios corresponding to each dimension\n",
    "    ratios = []\n",
    "    for o in range(len(gt_pose)):\n",
    "        ratio = gt_sizes[o]/est_sizes[o]\n",
    "        ratios.append(sum(ratio)/len(ratio))\n",
    "\n",
    "    ## find which is better\n",
    "    outliers1 = outlier_detection(ratios, 25, 50)\n",
    "    ratio1 = [x for x in ratios if x not in outliers1]\n",
    "    ratio_obj_1 = sum(ratio1)/len(ratio1)\n",
    "\n",
    "    outliers2 = outlier_detection(ratios, 25, 75)\n",
    "    ratio2 = [x for x in ratios if x not in outliers2]\n",
    "    ratio_obj_2 = sum(ratio2)/len(ratio2)\n",
    "\n",
    "    ## two hypothesis\n",
    "    # print(ratio_obj_1, ratio_obj_2)\n",
    "    \n",
    "    ## USING TRAJECTORY DIMENSIONS\n",
    "    ## ratios of last count number of trajectories\n",
    "    ## dividing x,y,z of ground truth trajectory and estimated trajectory\n",
    "    count = 50\n",
    "    trans_ratios = []\n",
    "    for t in range(1, count+1):\n",
    "        trans_ratios.append(origin_gt_traj[-t][:3, -1]/origin_aligned_traj[-t][:3, -1])\n",
    "\n",
    "    trans_ratios = np.array(trans_ratios)\n",
    "\n",
    "    ## 2nd dimension is depth which is very much erroneous\n",
    "    ## taking x and y which is first and last dimensions\n",
    "    ratio_traj_1 = np.mean(trans_ratios, axis=0)[0]\n",
    "    ratio_traj_2 = np.mean(trans_ratios, axis=0)[2]\n",
    "    ratio_traj_3 = (ratio_traj_1 + ratio_traj_2)/2\n",
    "\n",
    "    ## three hypothesis\n",
    "    # print(ratio_traj_1, ratio_traj_2, ratio_traj_3)\n",
    "    \n",
    "    # ratio_obj_1, ratio_obj_2, ratio_traj_1, ratio_traj_2, ratio_traj_3\n",
    "    # The ratios obtained from objects are better than the ratio from trajcetories\n",
    "    scale = ratio_obj_2 ## seems to have better object error metrics\n",
    "    \n",
    "    ## Scaling the estimated camera pose\n",
    "    scaled_origin_aligned_traj = []\n",
    "\n",
    "    for traj in origin_aligned_traj:\n",
    "        updated_translation = traj[:3, -1] * scale\n",
    "        T = np.eye(4)\n",
    "        T[:3, :3] = traj[:3, :3]\n",
    "        T[:3, -1] = updated_translation\n",
    "        scaled_origin_aligned_traj.append(T)\n",
    "        \n",
    "    ## Scaling the estimated object pose\n",
    "    scaled_origin_aligned_quad = []\n",
    "\n",
    "    for o in range(len(origin_aligned_quad)):\n",
    "        temp = dict()\n",
    "\n",
    "        temp['quadric_id'] = origin_aligned_quad[o]['quadric_id']\n",
    "\n",
    "        class_id = origin_aligned_quad[o]['category_id']\n",
    "        temp['category_id'] = class_id\n",
    "\n",
    "        # pose, radii\n",
    "        pose = origin_aligned_quad[o]['pose']\n",
    "        pose[:3, -1] *= scale\n",
    "        temp['pose'] = pose\n",
    "\n",
    "        radii = origin_aligned_quad[o]['radii']\n",
    "        radii *= scale\n",
    "        temp['radii'] = radii\n",
    "\n",
    "        scaled_origin_aligned_quad.append(temp)\n",
    "        \n",
    "    ## Aligning the estimated camera pose with ground truth\n",
    "    aligned_traj = []\n",
    "\n",
    "    for traj in scaled_origin_aligned_traj:\n",
    "        aligned_traj.append(np.dot(gt_traj[0], traj))\n",
    "        \n",
    "    ## Further alignment using OPEN3D - ICP\n",
    "    \n",
    "    # Get the 3D points\n",
    "    points_gt = []\n",
    "    points_est = []\n",
    "\n",
    "    for i in range(len(gt_traj)):\n",
    "        points_gt.append(list(gt_traj[i][:3, 3]))\n",
    "        points_est.append(list(aligned_traj[i][:3, 3]))\n",
    "\n",
    "    points_gt = np.array(points_gt)\n",
    "    points_est = np.array(points_est)\n",
    "\n",
    "    # Define or load your 3D point clouds as numpy arrays: cloud_source_np and cloud_target_np\n",
    "\n",
    "    # Convert numpy arrays to Open3D point cloud objects\n",
    "    cloud_source = o3d.geometry.PointCloud()\n",
    "    cloud_source.points = o3d.utility.Vector3dVector(points_est)\n",
    "\n",
    "    cloud_target = o3d.geometry.PointCloud()\n",
    "    cloud_target.points = o3d.utility.Vector3dVector(points_gt)\n",
    "\n",
    "    # Perform ICP registration\n",
    "    icp_result = o3d.pipelines.registration.registration_icp(\n",
    "        cloud_source, cloud_target, max_correspondence_distance=1000,\n",
    "        estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "    # Get the optimized transformation matrix\n",
    "    transformation_matrix = icp_result.transformation\n",
    "    \n",
    "    ## applying transformation on the estimated trajectory\n",
    "\n",
    "    for x in range(len(aligned_traj)):\n",
    "        aligned_traj[x] = np.dot(transformation_matrix, aligned_traj[x])\n",
    "        \n",
    "    ## Aligning the estimated object pose with ground truth\n",
    "    aligned_quad = []\n",
    "\n",
    "    for o in range(len(scaled_origin_aligned_quad)):\n",
    "        temp = dict()\n",
    "\n",
    "        temp['quadric_id'] = scaled_origin_aligned_quad[o]['quadric_id']\n",
    "\n",
    "        class_id = scaled_origin_aligned_quad[o]['category_id']\n",
    "        temp['category_id'] = class_id\n",
    "\n",
    "        # pose, radii\n",
    "        pose = scaled_origin_aligned_quad[o]['pose']\n",
    "        temp['pose'] = np.dot(gt_traj[0], pose)## first alignment with ground truth\n",
    "        temp['pose'] = np.dot(transformation_matrix, temp['pose']) ## second alignment with ICP\n",
    "\n",
    "        temp['radii'] = scaled_origin_aligned_quad[o]['radii']\n",
    "\n",
    "        aligned_quad.append(temp)\n",
    "    ################# ESTIMATED - ALIGNED POSES #################\n",
    "    \n",
    "    ################# ESTIMATED TRAJECTORY PLOTTING #################\n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X Axis')\n",
    "        ax.set_ylabel('Y Axis')\n",
    "        ax.set_zlabel('Z Axis')\n",
    "        ax.set_xlim((x_min-100, x_max+100))\n",
    "        ax.set_ylim((y_min-100, y_max+100))\n",
    "        ax.set_zlim((z_min-100, z_max+100))\n",
    "\n",
    "        plot_traj(gt_traj, ax, c = 'blue', label=\"ground truth\", length=200)\n",
    "        plot_traj(aligned_traj, ax, c = 'red', label=\"estimated\", length=200)\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(False)\n",
    "        plt.savefig(images_save_directory + '/aligned_trajectories.png')\n",
    "        plt.close()\n",
    "    ################# ESTIMATED TRAJECTORY PLOTTING #################\n",
    "    \n",
    "    ################# ESTIMATED SCENE PLOTTING #################\n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X Axis')\n",
    "        ax.set_ylabel('Y Axis')\n",
    "        ax.set_zlabel('Z Axis')\n",
    "        ax.set_xlim((x_min-100, x_max+100))\n",
    "        ax.set_ylim((y_min-100, y_max+100))\n",
    "        ax.set_zlim((z_min-100, z_max+100))\n",
    "\n",
    "        ## plotting trajectories\n",
    "        plot_traj(gt_traj, ax, c = 'blue', label=\"ground truth\", length=200)\n",
    "        plot_traj(aligned_traj, ax, c = 'red', label=\"estimated\", length=200)\n",
    "\n",
    "        # plotting GT object pose as cuboid\n",
    "        for o in range(len(object_pose['1'])):\n",
    "            plot_traj([gt_pose[o]], ax, length=200)\n",
    "\n",
    "            class_id = str(classes[o])\n",
    "            radii = np.array([model_data[class_id]['size_x'],\n",
    "                              model_data[class_id]['size_y'],\n",
    "                              model_data[class_id]['size_z']])\n",
    "            # plotting as ellipsoids\n",
    "            # plot_ellipsoid(gt_pose[o], radii/2, ax, colors[o])\n",
    "            # plotting as cuboids\n",
    "            plot_cuboid(gt_pose[o], radii, ax, colors[o])\n",
    "\n",
    "            # plotting estimated object pose as ellipsoids\n",
    "            class_id = aligned_quad[o]['category_id']\n",
    "            plot_traj([aligned_quad[o]['pose']], ax, length=200)\n",
    "\n",
    "            plot_ellipsoid(aligned_quad[o]['pose'], aligned_quad[o]['radii'],\n",
    "                           ax, colors[classes.index(class_id)])\n",
    "\n",
    "\n",
    "        ax.legend(handles=[\n",
    "                Patch(facecolor=c, edgecolor=c, label=dataset_info[str(l)]) for l, c in zip(classes, colors)\n",
    "            ])\n",
    "        ax.grid(False)\n",
    "        plt.savefig(images_save_directory + '/estimated_scene.png')\n",
    "        plt.close()\n",
    "        \n",
    "    ################# ESTIMATED SCENE PLOTTING #################\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    ################# ERROR METRICS CALCULATION #################\n",
    "    \n",
    "    ####### CAMERA POSE ERRORS #######\n",
    "    ## 1. Root Mean Square Error(RMSE) for Trajectory Deviation (Euclidean Distance)\n",
    "    # Euclidean distance\n",
    "    euc_errors = []\n",
    "\n",
    "    for i in range(len(gt_traj)):\n",
    "        euc_errors.append(euclidean_distance(gt_traj[i][:3, 3], aligned_traj[i][:3, 3]))\n",
    "        \n",
    "    squared_error = [item ** 2 for item in euc_errors]\n",
    "    \n",
    "    mean_squared_error = sum(squared_error)/len(squared_error)\n",
    "    \n",
    "    rmse = math.sqrt(mean_squared_error)\n",
    "    \n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot([i for i in range(len(euc_errors))], euc_errors)\n",
    "        ax.set_xlabel('Camera Frame')\n",
    "        ax.set_ylabel('Euclidean Distance (mm)')\n",
    "        plt.title(\"Trajectory Deviation\")\n",
    "        ax.grid(False)\n",
    "        plt.savefig(images_save_directory + '/trajectory_deviation.png')\n",
    "        plt.close()\n",
    "        \n",
    "    ## 2. Average Rotation Error for Camera Poses\n",
    "    rot_errors_traj = []\n",
    "    \n",
    "    for i in range(len(gt_traj)):\n",
    "        # order is dependent\n",
    "        rot_errors_traj.append(rotation_error_quaternion(matrix_to_quaternion(gt_traj[i]),\n",
    "                                                         matrix_to_quaternion(aligned_traj[i])))\n",
    "        \n",
    "    rot_errors_traj = [value for value in rot_errors_traj if not math.isnan(value)]\n",
    "    \n",
    "    avg_rot_error_traj = sum(rot_errors_traj)/len(rot_errors_traj)\n",
    "    \n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot([i for i in range(len(rot_errors_traj))], rot_errors_traj)\n",
    "        ax.set_xlabel('Camera Frame')\n",
    "        ax.set_ylabel('Rotation error (rad)')\n",
    "        plt.title(\"Rotation Error\")\n",
    "        ax.grid(False)\n",
    "        plt.savefig(images_save_directory + '/trajectory_rotation_error.png')\n",
    "        plt.close()\n",
    "    \n",
    "    ## 3. Disparity Measure by Procrustes Analysis\n",
    "    points_gt = []\n",
    "    points_est = []\n",
    "\n",
    "    for i in range(len(gt_traj)):\n",
    "        points_gt.append(list(gt_traj[i][:3, 3]))\n",
    "        points_est.append(list(aligned_traj[i][:3, 3]))\n",
    "\n",
    "    points_gt = np.array(points_gt)\n",
    "    points_est = np.array(points_est)\n",
    "\n",
    "    set1, set2, disparity = procrustes_analysis(points_gt, points_est)\n",
    "    \n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot(set1[:,0], set1[:,1], set1[:,2], c='b', marker='o', alpha=0.5, label='Ground Truth')\n",
    "        ax.plot(set2[:,0], set2[:,1], set2[:,2], c='r', marker='o', alpha=0.5, label='Estimated')\n",
    "        ax.grid(False)\n",
    "        ax.legend()\n",
    "        plt.title(\"Procrustes Analysis - Normalised and Aligned\")\n",
    "        plt.savefig(images_save_directory + '/procrustes_analysis.png')\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    ## 4. Fréchet Distance\n",
    "    f_dist = frechet_distance(points_gt, points_est)\n",
    "    \n",
    "    ## 5. Chamfer Distance\n",
    "    c_dist = chamfer_distance(points_gt, points_est)\n",
    "    ####### CAMERA POSE ERRORS #######\n",
    "    \n",
    "    ####### OBJECT POSE ERRORS #######\n",
    "    ## 1. Average Centroid Error (Euclidean Distance)\n",
    "    # GT centroids\n",
    "    centroids_gt = []\n",
    "    for o in range(len(gt_pose)):\n",
    "        centroids_gt.append(gt_pose[o][:3,3])\n",
    "\n",
    "    # Estimated centroids\n",
    "    centroids_est = []\n",
    "    for o in range(len(aligned_quad)):\n",
    "        centroids_est.append(aligned_quad[o]['pose'][:3,3])\n",
    "\n",
    "    centroid_errors = []\n",
    "\n",
    "    for i in range(len(centroids_gt)):\n",
    "        centroid_errors.append(euclidean_distance(centroids_gt[i], centroids_est[i]))\n",
    "    # average centroid error\n",
    "    avg_cen_err = sum(centroid_errors)/len(centroid_errors)\n",
    "    \n",
    "    \n",
    "    ## 2. Average Rotation Error for Object Poses\n",
    "    \n",
    "    # rotation matrix quadrent correction\n",
    "    est_obj_mod_mat = []\n",
    "\n",
    "    for i in range(len(centroids_gt)):\n",
    "        inverse = np.eye(4)\n",
    "        inverse[:3, :3] = aligned_quad[i]['pose'][:3, :3].T\n",
    "\n",
    "        # Convert the rotation matrix to a Rotation object\n",
    "        r = Rotation.from_matrix(inverse[:3, :3])\n",
    "\n",
    "        # Convert to Euler angles (XYZ rotation order)\n",
    "        euler_angles = r.as_euler('xyz', degrees=True)\n",
    "\n",
    "        # Extract individual angles\n",
    "        alpha = threshold_to_closest_multiple_of_90(euler_angles[0])  # X-axis rotation\n",
    "        beta = threshold_to_closest_multiple_of_90(euler_angles[1])   # Y-axis rotation\n",
    "        gamma = threshold_to_closest_multiple_of_90(euler_angles[2])  # Z-axis rotation\n",
    "\n",
    "        rotation_transform = np.eye(4)\n",
    "        # Create a Rotation matrix from Euler angles (XYZ rotation order)\n",
    "        r = Rotation.from_euler('xyz', [alpha, beta, gamma], degrees=True)\n",
    "        rotation_transform[:3, :3] = r.as_matrix()\n",
    "\n",
    "        est_obj_mod_mat.append(np.dot(aligned_quad[i]['pose'], rotation_transform))\n",
    "        \n",
    "    # computing error\n",
    "    rot_errors_obj = []\n",
    "\n",
    "    for o in range(len(gt_pose)):\n",
    "        # order is dependent\n",
    "        rot_errors_obj.append(rotation_error_quaternion(matrix_to_quaternion(gt_pose[o]),\n",
    "                                                        matrix_to_quaternion(est_obj_mod_mat[o])))\n",
    "\n",
    "    \n",
    "    avg_rot_error_obj = sum(rot_errors_obj)/ len(rot_errors_obj)\n",
    "\n",
    "    \n",
    "    ## 3. Percentage of Intersection Volume\n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X Axis')\n",
    "        ax.set_ylabel('Y Axis')\n",
    "        ax.set_zlabel('Z Axis')\n",
    "        ax.grid(False)\n",
    "        ax.legend(handles=[\n",
    "            Patch(facecolor=c, edgecolor=c, label=dataset_info[str(l)]) for l, c in zip(classes, colors)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    # to ensure same particles are generated for oa-slam and quadricslam\n",
    "    np.random.seed(50)\n",
    "\n",
    "    ## The higher the nmber of points, the better is the monte-carlo approximation\n",
    "    num_points = 10000\n",
    "\n",
    "    overlap_cuboid = [] # percentage of overlapping points\n",
    "\n",
    "    volume_cuboid = []\n",
    "    volume_est_ellipsoid = []\n",
    "    \n",
    "\n",
    "    for o in range(len(gt_pose)):\n",
    "        \n",
    "        class_id = str(classes[o])\n",
    "        # volume of a cuboid\n",
    "        cube_dimensions = np.array([model_data[class_id]['size_x'],\n",
    "                                    model_data[class_id]['size_y'],\n",
    "                                    model_data[class_id]['size_z']])\n",
    "        # length * breadth * height\n",
    "        volume_cuboid.append(np.prod(cube_dimensions))\n",
    "\n",
    "        # volume of an ellipsoid\n",
    "        volume_est_ellipsoid.append((4/3) * np.pi * (np.prod(aligned_quad[o]['radii'])))\n",
    "\n",
    "        # generate points within the cuboid\n",
    "        points_within_cuboid = generate_points_within_cuboid(gt_pose[o][:3,3], cube_dimensions, \n",
    "                                                             gt_pose[o][:3,:3], num_points)\n",
    "        points = np.array(points_within_cuboid)\n",
    "\n",
    "        # check if the point is within the ellipsoid\n",
    "        inside_est_ellipsoid = 0\n",
    "\n",
    "        for i in range(len(points)):\n",
    "            if is_point_inside_ellipsoid(points[i], aligned_quad[o]['pose'][:3,3],\n",
    "                                         aligned_quad[o]['radii'], aligned_quad[o]['pose'][:3,:3]):\n",
    "                inside_est_ellipsoid+=1\n",
    "\n",
    "        overlap_cuboid.append(inside_est_ellipsoid / num_points)\n",
    "        \n",
    "        if plottings:\n",
    "            ### Plotting the experiment ###\n",
    "            # scatter the points within the plot\n",
    "            ax.scatter(points[:, 0], points[:, 1], points[:, 2], c='k', s=0.1)\n",
    "            # plot the GT cuboid\n",
    "            plot_cuboid(gt_pose[o], cube_dimensions, ax, colors[classes.index(classes[o])])\n",
    "            # plot the estimated ellipsoid\n",
    "            plot_ellipsoid(aligned_quad[o]['pose'], aligned_quad[o]['radii'], ax,\n",
    "                           colors[classes.index(classes[o])])\n",
    "        \n",
    "    \n",
    "    if plottings:\n",
    "        plt.title(\"Monte-Carlo Sample Point Generation\")\n",
    "        plt.savefig(images_save_directory + '/intersection_volume.png')\n",
    "        plt.close()\n",
    "        \n",
    "    intersection_percent = [] # percentage of intersection\n",
    "    for i in range(len(overlap_cuboid)):\n",
    "        intersection_percent.append((2*volume_cuboid[i]*overlap_cuboid[i]*100)/(volume_cuboid[i]+volume_est_ellipsoid[i]))\n",
    "        \n",
    "    avg_int_per = sum(intersection_percent)/len(intersection_percent)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 4. Percentage of Aligned Intersection Volume\n",
    "    if plottings:\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X Axis')\n",
    "        ax.set_ylabel('Y Axis')\n",
    "        ax.set_zlabel('Z Axis')\n",
    "        ax.grid(False)\n",
    "        ax.legend(handles=[\n",
    "            Patch(facecolor=c, edgecolor=c, label=dataset_info[str(l)]) for l, c in zip(classes, colors)\n",
    "        ])\n",
    "    overlap_cuboid = [] # percentage of overlapping points\n",
    "\n",
    "    volume_cuboid = []\n",
    "    volume_est_ellipsoid = []\n",
    "    \n",
    "\n",
    "    for o in range(len(gt_pose)):\n",
    "        \n",
    "        class_id = str(classes[o])\n",
    "        # volume of a cuboid\n",
    "        cube_dimensions = np.array([model_data[class_id]['size_x'],\n",
    "                                    model_data[class_id]['size_y'],\n",
    "                                    model_data[class_id]['size_z']])\n",
    "        # length * breadth * height\n",
    "        volume_cuboid.append(np.prod(cube_dimensions))\n",
    "\n",
    "        # volume of an ellipsoid\n",
    "        volume_est_ellipsoid.append((4/3) * np.pi * (np.prod(aligned_quad[o]['radii'])))\n",
    "\n",
    "        # generate points within the cuboid\n",
    "        points_within_cuboid = generate_points_within_cuboid(gt_pose[o][:3,3], cube_dimensions, \n",
    "                                                             gt_pose[o][:3,:3], num_points)\n",
    "        points = np.array(points_within_cuboid)\n",
    "\n",
    "        # check if the point is within the ellipsoid\n",
    "        inside_est_ellipsoid = 0\n",
    "        \n",
    "        # Estimated object aligning with GT object\n",
    "        gt_inv = np.linalg.inv(gt_pose[o])\n",
    "        T_rel = np.dot(gt_inv, aligned_quad[o]['pose'])\n",
    "        aligned_est = np.dot(aligned_quad[o]['pose'], np.linalg.inv(T_rel)) ## NEW POSE\n",
    "        # Now the length, breadth and height may interchange.\n",
    "        sorted_indices = np.argsort(cube_dimensions)\n",
    "        sorted_array = np.sort(aligned_quad[o]['radii'])\n",
    "        aligned_size = np.ones(3)\n",
    "        for j in range(3):\n",
    "            aligned_size[sorted_indices[j]] = sorted_array[j] ## NEW RADIUS\n",
    "\n",
    "        for i in range(len(points)):\n",
    "            if is_point_inside_ellipsoid(points[i], aligned_est[:3,3],\n",
    "                                         aligned_size, aligned_est[:3,:3]):\n",
    "                inside_est_ellipsoid+=1\n",
    "\n",
    "        overlap_cuboid.append(inside_est_ellipsoid / num_points)\n",
    "        \n",
    "        if plottings:\n",
    "            ### Plotting the experiment ###\n",
    "            # scatter the points within the plot\n",
    "            ax.scatter(points[:, 0], points[:, 1], points[:, 2], c='k', s=0.1)\n",
    "            # plot the GT cuboid\n",
    "            plot_cuboid(gt_pose[o], cube_dimensions, ax, colors[classes.index(classes[o])])\n",
    "            # plot the estimated ellipsoid\n",
    "            plot_ellipsoid(aligned_est, aligned_size, ax,\n",
    "                           colors[classes.index(classes[o])])\n",
    "        \n",
    "    \n",
    "    if plottings:\n",
    "        plt.title(\"Monte-Carlo Sample Point Generation After Alignment\")\n",
    "        plt.savefig(images_save_directory + '/intersection_volume_after_alignment.png')\n",
    "        plt.close()\n",
    "        \n",
    "    intersection_percent_aligned = [] # percentage of intersection after alignment\n",
    "    for i in range(len(overlap_cuboid)):\n",
    "        intersection_percent_aligned.append((2*volume_cuboid[i]*overlap_cuboid[i]*100)/(volume_cuboid[i]+volume_est_ellipsoid[i]))\n",
    "        \n",
    "    avg_int_per_aligned = sum(intersection_percent_aligned)/len(intersection_percent_aligned)\n",
    "    \n",
    "    ####### OBJECT POSE ERRORS #######\n",
    "    \n",
    "    ################# ERROR METRICS CALCULATION #################\n",
    "    \n",
    "    ################# EXPORTING TO JSON #################\n",
    "    labels = [dataset_info[str(l)] for l in classes]\n",
    "    \n",
    "    export_data = {'camera_pose': {\n",
    "                                   'euc_error': euc_errors,\n",
    "                                   'root_mean_square_error': rmse,\n",
    "                                   'rotation_error': rot_errors_traj,\n",
    "                                   'average_rotation_error': avg_rot_error_traj,\n",
    "                                   'disparity': disparity,\n",
    "                                   'frechet_distance': f_dist,\n",
    "                                   'chamfer_distance': c_dist\n",
    "                                    },\n",
    "                   'object_pose': {\n",
    "                                   'centroid_error': centroid_errors,\n",
    "                                   'average_centroid_error': avg_cen_err,\n",
    "                                   'rotation_error': rot_errors_obj,\n",
    "                                   'average_rotation_error': avg_rot_error_obj,\n",
    "                                   'volume_of_intersection': intersection_percent,\n",
    "                                   'average_volume_of_intersection': avg_int_per,\n",
    "                                   'volume_of_intersection_aligned': intersection_percent_aligned,\n",
    "                                   'average_volume_of_intersection_aligned': avg_int_per_aligned,\n",
    "                                   'labels': labels\n",
    "                                    }\n",
    "                   }\n",
    "\n",
    "    export_path = os.path.join(dataset, 'oa_slam_result/error_metrics.json')\n",
    "        \n",
    "    with open(export_path, \"w\") as json_file:\n",
    "        json.dump(export_data, json_file, indent=4)  # indent for pretty formatting\n",
    "    ################# EXPORTING TO JSON #################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b179f7",
   "metadata": {},
   "source": [
    "# Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae90466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000001\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000002\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000003\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000004\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000005\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000006\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000007\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000008\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000009\n",
      "Processed:  /home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/000010\n"
     ]
    }
   ],
   "source": [
    "##### Dataset path\n",
    "dataset_path = \"/home/allen/Desktop/RnD_Github/AllenIsaacRnD/dataset/\"\n",
    "\n",
    "##### Extracting the absolute path for all the datasets\n",
    "datasets_path = []\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "        datasets_path.append(folder_path)\n",
    "        \n",
    "datasets_path.sort()\n",
    "        \n",
    "##### Processing each dataset\n",
    "for dataset in datasets_path:\n",
    "    \n",
    "    postprocessing(dataset, True)\n",
    "    print(\"Processed: \", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eec484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
