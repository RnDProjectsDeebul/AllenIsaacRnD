@misc{sünderhauf2017dual,
      title={{Dual Quadrics from Object Detection BoundingBoxes as Landmark Representations in SLAM}}, 
      author={Niko Sünderhauf and Michael Milford},
      year={2017},
      eprint={1708.00965},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@ARTICLE{7919240,
  author={Rubino, Cosimo and Crocco, Marco and Del Bue, Alessio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={3D Object Localisation from Multi-View Image Detections}, 
  year={2018},
  volume={40},
  number={6},
  pages={1281-1294},
  doi={10.1109/TPAMI.2017.2701373}}

  @ARTICLE{orbslam,
  author={Mur-Artal, Raúl and Montiel, J. M. M. and Tardós, Juan D.},
  journal={IEEE Transactions on Robotics}, 
  title={{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}}, 
  year={2015},
  volume={31},
  number={5},
  pages={1147-1163},
  doi={10.1109/TRO.2015.2463671}}

@INPROCEEDINGS{linesegment,
  author={Lemaire, Thomas and Lacroix, Simon},
  booktitle={Proceedings 2007 IEEE International Conference on Robotics and Automation}, 
  title={{Monocular-vision based SLAM using Line Segments}}, 
  year={2007},
  volume={},
  number={},
  pages={2791-2796},
  doi={10.1109/ROBOT.2007.363894}}

  @INPROCEEDINGS{infiniteplanes,
  author={Kaess, Michael},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={{Simultaneous localization and mapping with infinite planes}}, 
  year={2015},
  volume={},
  number={},
  pages={4605-4611},
  doi={10.1109/ICRA.2015.7139837}}

  @INPROCEEDINGS{slam++,
  author={Salas-Moreno, Renato F. and Newcombe, Richard A. and Strasdat, Hauke and Kelly, Paul H.J. and Davison, Andrew J.},
  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={{SLAM++: Simultaneous Localisation and Mapping at the Level of Objects}}, 
  year={2013},
  volume={},
  number={},
  pages={1352-1359},
  doi={10.1109/CVPR.2013.178}}

@INPROCEEDINGS{semanticfusion,
  author={McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={SemanticFusion: Dense 3D semantic mapping with convolutional neural networks}, 
  year={2017},
  volume={},
  number={},
  pages={4628-4635},
  doi={10.1109/ICRA.2017.7989538}}

@MastersThesis{dataassosciation,
    author     =     {Kamil Kaminski},
    title     =     {{Data association for object-based SLAM}},
    school     =     {KTH Royal Institute of Technology},
    address     =     {Stockholm, Sverige},
    year     =     {2020},
    }

@misc{gtsam,
  author       = {Frank Dellaert and GTSAM Contributors},
  title        = {{Georgia Tech Borg Lab - GTSAM}},
  month        = May,
  year         = 2022,
  publisher    = {Georgia Tech Borg Lab},
  version      = {4.2a8},
  doi          = {10.5281/zenodo.5794541},
  howpublished = {\url{https://github.com/borglab/gtsam}},
  url          = {https://github.com/borglab/gtsam}}


@article{orientation_factor,
  title={{An Orientation Factor for Object-Oriented SLAM}},
  author={Natalie Jablonsky and Michael Milford and Niko S{\"u}nderhauf},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.06977}
}

@INPROCEEDINGS{rgbd,
  author={Sturm, Jürgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={{A benchmark for the evaluation of RGB-D SLAM systems}}, 
  year={2012},
  volume={},
  number={},
  pages={573-580},
  abstract={In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
  keywords={},
  doi={10.1109/IROS.2012.6385773},
  ISSN={2153-0866},
  month={Oct},}

@ARTICLE{quadric2,
  author={Nicholson, Lachlan and Milford, Michael and Sünderhauf, Niko},
  journal={IEEE Robotics and Automation Letters}, 
  title={{QuadricSLAM: Dual Quadrics From Object Detections as Landmarks in Object-Oriented SLAM}}, 
  year={2019},
  volume={4},
  number={1},
  pages={1-8},
  abstract={In this letter, we use two-dimensional (2-D) object detections from multiple views to simultaneously estimate a 3-D quadric surface for each object and localize the camera position. We derive a simultaneous localization and mapping (SLAM) formulation that uses dual quadrics as 3-D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2-D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
  keywords={},
  doi={10.1109/LRA.2018.2866205},
  ISSN={2377-3766},
  month={Jan},}


@INPROCEEDINGS{oaslam,
  author={Zins, Matthieu and Simon, Gilles and Berger, Marie-Odile},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={OA-SLAM: Leveraging Objects for Camera Relocalization in Visual SLAM}, 
  year={2022},
  volume={},
  number={},
  pages={720-728},
  abstract={In this work, we explore the use of objects in Simultaneous Localization and Mapping in unseen worlds and propose an object-aided system (OA-SLAM). More precisely, we show that, compared to low-level points, the major benefit of objects lies in their higher-level semantic and discriminating power. Points, on the contrary, have a better spatial localization accuracy than the generic coarse models used to represent objects (cuboid or ellipsoid). We show that combining points and objects is of great interest to address the problem of camera pose recovery. Our main contributions are: (1) we improve the relocalization ability of a SLAM system using high-level object landmarks; (2) we build an automatic system, capable of identifying, tracking and reconstructing objects with 3D ellipsoids; (3) we show that object-based localization can be used to reinitialize or resume camera tracking. Our fully automatic system allows on-the-fly object mapping and enhanced pose tracking recovery, which we think, can significantly benefit to the AR community. Our experiments show that the camera can be relocalized from viewpoints where classical methods fail. We demonstrate that this localization allows a SLAM system to continue working despite a tracking loss, which can happen frequently with an uninitiated user. Our code and test data are released at gitlab.inria.fr/tangram/oa-slam.},
  keywords={},
  doi={10.1109/ISMAR55827.2022.00090},
  ISSN={1554-7868},
  month={Oct},}

@inproceedings{StructureAS,
  title={Structure Aware SLAM Using Quadrics and Planes},
  author={Mehdi Hosseinzadeh and Yasir Latif and Trung T. Pham and Niko S{\"u}nderhauf and Ian D. Reid},
  booktitle={Asian Conference on Computer Vision},
  year={2018}
}

@INPROCEEDINGS{infpla1,
  author={Kaess, Michael},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Simultaneous localization and mapping with infinite planes}, 
  year={2015},
  volume={},
  number={},
  pages={4605-4611},
  abstract={Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods. We show how to include infinite planes into a least-squares formulation for mapping, using a homogeneous plane parametrization with a corresponding minimal representation for the optimization. Because it is a minimal representation, it is suitable for use with Gauss-Newton, Powell's Dog Leg and incremental solvers such as iSAM. We also introduce a relative plane formulation that improves convergence. We evaluate our proposed approach on simulated data to show its advantages over alternative solutions. We also introduce a simple mapping system and present experimental results, showing real-time mapping of select indoor environments with a hand-held RGB-D sensor.},
  keywords={},
  doi={10.1109/ICRA.2015.7139837},
  ISSN={1050-4729},
  month={May},}

@INPROCEEDINGS{infpla2,
  author={Taguchi, Yuichi and Jian, Yong-Dian and Ramalingam, Srikumar and Feng, Chen},
  booktitle={2013 IEEE International Conference on Robotics and Automation}, 
  title={Point-plane SLAM for hand-held 3D sensors}, 
  year={2013},
  volume={},
  number={},
  pages={5182-5189},
  abstract={We present a simultaneous localization and mapping (SLAM) algorithm for a hand-held 3D sensor that uses both points and planes as primitives. We show that it is possible to register 3D data in two different coordinate systems using any combination of three point/plane primitives (3 planes, 2 planes and 1 point, 1 plane and 2 points, and 3 points). Our algorithm uses the minimal set of primitives in a RANSAC framework to robustly compute correspondences and estimate the sensor pose. As the number of planes is significantly smaller than the number of points in typical 3D data, our RANSAC algorithm prefers primitive combinations involving more planes than points. In contrast to existing approaches that mainly use points for registration, our algorithm has the following advantages: (1) it enables faster correspondence search and registration due to the smaller number of plane primitives; (2) it produces plane-based 3D models that are more compact than point-based ones; and (3) being a global registration algorithm, our approach does not suffer from local minima or any initialization problems. Our experiments demonstrate real-time, interactive 3D reconstruction of indoor spaces using a hand-held Kinect sensor.},
  keywords={},
  doi={10.1109/ICRA.2013.6631318},
  ISSN={1050-4729},
  month={May},}

@inproceedings{effplaneseg,
  title={Efficient Organized Point Cloud Segmentation with Connected Components},
  author={Alexander J. B. Trevor and Suat Gedikli and Radu Bogdan Rusu and Henrik I. Christensen},
  booktitle={In: 3rd Workshop on Semantic Perception Mapping and Exploration (SPME), Karlsruhe, Germany},
  year={2013}
}